Checkout Flow Optimization
==============================

A rigorous A/B testing framework for evaluating checkout flow improvements

## Project Overview

E-commerce checkout abandonment costs businesses billions annually. When users add items to their cart but fail to complete the purchase, we lose revenue and miss opportunities to serve customers. This project provides a complete experimentation framework to test whether a redesigned checkout flow increases the **Conditional Conversion Rate (CCR)**, the percentage of users who add to cart and successfully complete their order. We use synthetic data to demonstrate best practices in A/B testing: statistical rigor, guardrail monitoring, power analysis, and clear decision frameworks. The goal is to make data-driven launch decisions that balance revenue growth with user experience quality.

## Data Pipeline Summary

The pipeline generates synthetic checkout funnel events (add to cart, begin checkout, payment attempts, order completion) and stores them as date-partitioned Parquet files in `data/raw/`. Events flow into **DuckDB**, an embedded analytical database, where SQL views in `sql/schema.sql` expose the raw data. Analytical **marts** (`sql/marts/`) aggregate events into user-level metrics (experiment assignments, checkout steps, orders) optimized for fast queries. The simulator (`src/data/simulate.py`) allows configurable traffic volumes, baseline conversion rates, and treatment effects to test different scenarios. This architecture mirrors production data warehouses while remaining lightweight and reproducible.

## Key Analyses

1. **Funnel Analysis** — Track user progression through checkout steps (add to cart → begin checkout → payment → order completion) to identify drop-off points and friction
2. **Conditional Conversion Rate (CCR)** — Primary metric comparing treatment vs control using two-proportion z-tests with 95% confidence intervals
3. **Guardrail Metrics** — Ensure changes don't harm payment authorization rate, average order value, or latency; automatic pass/fail evaluation against thresholds
4. **Sensitivity Analysis** — Power analysis sweeping sample sizes and effect sizes to determine minimum detectable effects and recommend experiment durations

## Quick Start

```bash
# 1. Setup environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 2. Run complete pipeline (generates data, builds warehouse, runs analysis)
make pipeline

# 3. Launch interactive dashboard
make app
# Visit http://localhost:8501 to explore results

# Optional: Run sensitivity analysis for power calculations
make sensitivity
```

## How to Read the Results

- **Quick view:** See [reports/REPORT.md](reports/REPORT.md) for the latest experiment results, statistical tests, and executive summary
- **Full analysis:** Open [notebooks/experiment_readout.ipynb](notebooks/experiment_readout.ipynb) for comprehensive readout with decision framework
- **Data source:** All data are synthetic, generated by `src/data/simulate.py` and stored in `data/raw/` as partitioned Parquet files

## Project Organization
------------

```
├── Makefile              <- Commands: `make pipeline`, `make app`, `make sensitivity`
├── README.md             <- This file
├── requirements.txt      <- Python dependencies
│
├── configs/              <- Experiment and analysis configuration
│   ├── experiment.yml    <- A/B test parameters (MDE, variants, guardrails)
│   ├── tracking_plan.yml <- Event schemas and data types
│   └── sensitivity_presets.yml <- Power analysis parameter presets
│
├── data/
│   └── raw/              <- Partitioned Parquet files by event type and date
│       ├── add_to_cart/
│       ├── begin_checkout/
│       ├── checkout_step_view/
│       ├── payment_attempt/
│       └── order_completed/
│
├── duckdb/
│   └── warehouse.duckdb  <- Embedded analytical database
│
├── sql/
│   ├── schema.sql        <- Views over raw Parquet data
│   └── marts/            <- Aggregated analytical tables
│       ├── fct_experiments.sql  <- User-level experiment assignments
│       ├── fct_checkout_steps.sql <- Checkout step metrics
│       └── fct_orders.sql       <- Completed orders
│
├── src/                  <- Source code
│   ├── data/
│   │   └── simulate.py   <- Generate synthetic checkout events
│   ├── analysis/
│   │   ├── stats_framework.py <- Statistical tests (z-test, CI, guardrails)
│   │   ├── metrics_runner.py  <- Query DuckDB for metric aggregates
│   │   ├── run_stats.py       <- Run statistical tests and print results
│   │   ├── save_results.py    <- Export results to JSON/CSV
│   │   └── sensitivity.py     <- Power analysis across parameter grid
│   ├── quality.py        <- Data quality checks (SRM, referential integrity)
│   ├── report.py         <- Generate markdown report with executive summary
│   ├── dashboard.py      <- Streamlit interactive dashboard
│   └── visualization/
│       └── app.py        <- Additional visualizations
│
├── notebooks/            <- Jupyter notebooks for analysis
│   ├── exploration.ipynb
│   ├── stats_sanity.ipynb
│   ├── power_sensitivity.ipynb
│   ├── diagnostics_deep_dive.ipynb
│   └── experiment_readout.ipynb <- Decision framework and ship/no-ship
│
├── reports/
│   ├── REPORT.md         <- Auto-generated experiment report
│   ├── results/          <- Statistical test outputs (JSON, CSV)
│   ├── templates/        <- Jinja2 templates for report generation
│   └── figures/          <- Charts and visualizations
│
└── references/           <- Documentation and checklists
    ├── metrics_spec.md
    ├── analysis_checklist.md
    ├── aa_validation_plan.md
    └── readout_narrative_checklist.md
```

---

**Note:** This project uses [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/) structure adapted for A/B testing workflows.
