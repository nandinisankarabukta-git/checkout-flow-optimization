{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical Analysis Sanity Check\n",
        "\n",
        "**Notebook:** stats_sanity  \n",
        "**Purpose:** Verify that `src/analysis/run_stats.py` produces correct statistical outputs and follows best practices.\n",
        "\n",
        "\n",
        "This notebook provides a sanity check for the statistical analysis framework used in the checkout flow optimization experiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CLI Output Structure\n",
        "\n",
        "The `python src/analysis/run_stats.py` command prints a compact statistical report with the following sections:\n",
        "\n",
        "### 1. Header\n",
        "- **Analysis Date:** Most recent date in the dataset\n",
        "\n",
        "### 2. Primary Metric: Conditional Conversion Rate (CCR)\n",
        "- **Per-Variant Rates:** Orders / Adders for control and treatment\n",
        "- **Effect Size (Absolute):** Difference in percentage points (pp)\n",
        "- **Effect Size (Relative):** Percentage change relative to control\n",
        "- **95% Confidence Interval:** Range for the absolute effect\n",
        "- **p-value:** Two-tailed significance test\n",
        "- **Significance Indicator:** ✓ SIGNIFICANT or ✗ NOT SIGNIFICANT at α=0.05\n",
        "\n",
        "### 3. Guardrail Metrics\n",
        "For each guardrail:\n",
        "- **Payment Authorization Rate:** Per-variant rates with 95% CIs\n",
        "- **Average Order Value (AOV):** Per-variant means with sample sizes\n",
        "- **Guardrail Status:** PASS or FAIL based on configured thresholds\n",
        "\n",
        "### 4. Decision\n",
        "- **Primary Metric Status:** Significant or not\n",
        "- **Guardrails Status:** All passed or one or more failed\n",
        "- **Recommendation:** SHIP or DO NOT SHIP\n",
        "\n",
        "### 5. Exit Code\n",
        "- **0:** Primary significant AND all guardrails pass\n",
        "- **1:** Primary not significant OR any guardrail fails\n",
        "- **2:** Error occurred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical Methods\n",
        "\n",
        "### Primary Metric: CCR (Conditional Conversion Rate)\n",
        "\n",
        "**Test:** Two-sample z-test for proportions\n",
        "\n",
        "**Method:**\n",
        "- Uses **pooled variance** for the standard error in hypothesis testing\n",
        "- Uses **unpooled variance** for confidence interval calculation\n",
        "- **95% Confidence Interval** for the absolute difference (treatment - control)\n",
        "- **Two-tailed p-value** using normal approximation\n",
        "\n",
        "**Formula:**\n",
        "```\n",
        "z = (p_treatment - p_control) / SE_pooled\n",
        "\n",
        "where SE_pooled = sqrt(p_pooled * (1 - p_pooled) * (1/n_control + 1/n_treatment))\n",
        "and p_pooled = (successes_control + successes_treatment) / (n_control + n_treatment)\n",
        "```\n",
        "\n",
        "**Assumptions:**\n",
        "- Large sample sizes (n > 30 per variant)\n",
        "- Independent observations\n",
        "- Random assignment to variants\n",
        "\n",
        "### Guardrail Metrics\n",
        "\n",
        "**Payment Authorization Rate:**\n",
        "- **Method:** Proportion confidence interval (Wald method)\n",
        "- **95% CI** for each variant independently\n",
        "- Comparison against baseline to check for drops\n",
        "\n",
        "**Average Order Value (AOV):**\n",
        "- **Method:** Mean confidence interval with normal approximation\n",
        "- Uses sample standard deviation and sample size\n",
        "- Suitable for large samples (n > 30)\n",
        "\n",
        "**Note:** For small samples or non-normal distributions, bootstrap methods would provide more robust confidence intervals. This is a future enhancement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Considerations\n",
        "\n",
        "### Multiple Testing Corrections\n",
        "\n",
        "**Current State:**\n",
        "- We test **one primary metric** (CCR) at α=0.05\n",
        "- Guardrails are **not** hypothesis tests; they are threshold checks\n",
        "- No multiple testing correction is currently applied\n",
        "\n",
        "**Future Considerations:**\n",
        "If you expand to test **multiple primary or secondary metrics** simultaneously, consider applying corrections for multiple comparisons to control the family-wise error rate (FWER) or false discovery rate (FDR):\n",
        "\n",
        "- **Bonferroni correction:** α_adjusted = α / number_of_tests (conservative)\n",
        "- **Holm-Bonferroni:** Sequential Bonferroni that's less conservative\n",
        "- **Benjamini-Hochberg (FDR):** Controls false discovery rate instead of FWER\n",
        "- **Pre-registration:** Declare primary metric before the experiment to avoid p-hacking\n",
        "\n",
        "**Current Approach:**\n",
        "- **Primary metric:** CCR is pre-specified in `configs/experiment.yml` and `references/metrics_spec.md`\n",
        "- **Guardrails:** Act as constraints, not hypothesis tests. They ensure we don't ship something that degrades critical metrics, even if CCR improves.\n",
        "- **Secondary metrics:** Reported for context only, not used for decision-making\n",
        "\n",
        "**Recommendation:** If you add more than one primary metric, revisit the significance threshold and apply appropriate corrections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Statistical Analysis CLI\n",
        "\n",
        "The following code cell will call the `run_stats.py` CLI via subprocess and capture the output for inspection and validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Placeholder: Call run_stats.py via subprocess and capture output\n",
        "# \n",
        "# import subprocess\n",
        "# import sys\n",
        "# from pathlib import Path\n",
        "# \n",
        "# # Ensure we're in the project root\n",
        "# project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "# \n",
        "# # Run the CLI\n",
        "# result = subprocess.run(\n",
        "#     [sys.executable, 'src/analysis/run_stats.py'],\n",
        "#     cwd=project_root,\n",
        "#     capture_output=True,\n",
        "#     text=True\n",
        "# )\n",
        "# \n",
        "# # Display output\n",
        "# print(result.stdout)\n",
        "# if result.stderr:\n",
        "#     print(\"STDERR:\", result.stderr)\n",
        "# \n",
        "# print(f\"\\nExit code: {result.returncode}\")\n",
        "\n",
        "# TODO: Implement CLI call and output capture\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
